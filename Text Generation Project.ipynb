{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "#import nltk\n",
    "#nltk.download('stopwords')\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data\n",
    "#Loading the data and opening our input data in the form of txt file\n",
    "file = open('frankenstein.txt').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization\n",
    "#Standardization\n",
    "def tokenize_words(input):\n",
    "    #Lowercasing the input\n",
    "    input = input.lower()\n",
    "    #Instantiating the tokenizer\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    #Tokenzing the text into tokens\n",
    "    tokens = tokenizer.tokenize(input)\n",
    "    #Filtering the stopwords using lambda\n",
    "    filtered = filter(lambda token: token not in stopwords.words('english'), tokens)\n",
    "    return \" \".join(filtered)\n",
    "\n",
    "#Preprocess the inputs and make tokens\n",
    "processed_inputs = tokenize_words(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chars to numbers\n",
    "#Coverts character in the input data into numbers\n",
    "#Sort all the characters in the input data and use enumerate fn\n",
    "#To get the numbers that represent the characters\n",
    "#Create a dict that stores the key and value i.e character and numbers\n",
    "chars = sorted(list(set(processed_inputs)))\n",
    "char_to_num = dict((c,i) for i,c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no of character: 269566\n",
      "Total vocab: 38\n"
     ]
    }
   ],
   "source": [
    "#Check if words to chars or chars to num has worked ?\n",
    "#To get an idea if it is working\n",
    "#Printing the length of variable\n",
    "input_len = len(processed_inputs)\n",
    "vocab_len = len(chars)\n",
    "print('Total no of character:', input_len)\n",
    "print(\"Total vocab:\", vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seq length\n",
    "#Defining the length of individual sequence\n",
    "#Individual Sequence is a complete mapping of input character as integers\n",
    "seq_length = 100\n",
    "x_data = []\n",
    "y_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Pattern: 269466\n"
     ]
    }
   ],
   "source": [
    "#Loop through the sequence\n",
    "#Going the loop and coverting character into numbers\n",
    "for i in range(0, input_len - seq_length, 1):\n",
    "    #Define the i/p and o/p sequence\n",
    "    #I/p is the current character plus the desired sequence length\n",
    "    in_seq = processed_inputs[i:i + seq_length]\n",
    "    #O/p is the initial character plus the total sequence length\n",
    "    out_seq = processed_inputs[i + seq_length]\n",
    "    #Converting the list of characters to integers absed on previous values and appending them to list\n",
    "    x_data.append([char_to_num[char] for char in in_seq])\n",
    "    y_data.append(char_to_num[out_seq])\n",
    "    \n",
    "n_patterns = len(x_data)\n",
    "print('Total Pattern:', n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert input sequence to np.array that our network can use\n",
    "x = np.reshape(x_data, (n_patterns, seq_length, 1))\n",
    "x = x/float(vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One hot encoding our label data\n",
    "y = np_utils.to_categorical(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the model\n",
    "#Creating the sequential model\n",
    "#Dropout is used to prevent overfitting\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape = (x.shape[1], x.shape[2]), return_sequences = True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256, return_sequences = True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile the model\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the weights\n",
    "filepath = 'model_weights_saved.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor = 'loss', verbose = 1, save_best_only = True, mode = 'min')\n",
    "desired_callback = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "269466/269466 [==============================] - 3471s 13ms/step - loss: 2.9399\n",
      "\n",
      "Epoch 00001: loss improved from inf to 2.93988, saving model to model_weights_saved.hdf5\n",
      "Epoch 2/4\n",
      "269466/269466 [==============================] - 4403s 16ms/step - loss: 2.9143\n",
      "\n",
      "Epoch 00002: loss improved from 2.93988 to 2.91434, saving model to model_weights_saved.hdf5\n",
      "Epoch 3/4\n",
      "269466/269466 [==============================] - 2750s 10ms/step - loss: 2.8888\n",
      "\n",
      "Epoch 00003: loss improved from 2.91434 to 2.88884, saving model to model_weights_saved.hdf5\n",
      "Epoch 4/4\n",
      "269466/269466 [==============================] - 2541s 9ms/step - loss: 2.6775\n",
      "\n",
      "Epoch 00004: loss improved from 2.88884 to 2.67754, saving model to model_weights_saved.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2ce3edb2808>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fit model\n",
    "model.fit(x, y, epochs = 4, batch_size = 256, callbacks = desired_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recompile the model with saved weights\n",
    "filename = 'model_weights_saved.hdf5'\n",
    "model.load_weights(filename)\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output of the model into character\n",
    "num_to_char = dict((i,c) for i,c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:\n",
      "\" olved remain father moment departure time turk renewed promise united deliverer felix remained expec \"\n"
     ]
    }
   ],
   "source": [
    "#Random seed to help generate\n",
    "start = np.random.randint(0, len(x_data) - 1)\n",
    "pattern = x_data[start]\n",
    "print(\"Random Seed:\")\n",
    "print(\"\\\"\", ''.join([num_to_char[value] for value in pattern]), \"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese sese"
     ]
    }
   ],
   "source": [
    "#Generate the text\n",
    "for i in range(1000):\n",
    "    X = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    X = X/float(vocab_len)\n",
    "    predictions = model.predict(X, verbose = 0)\n",
    "    index = np.argmax(predictions)\n",
    "    result = num_to_char[index]\n",
    "    seq_in = [num_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
